# NEO FlowOff PWA - Servidor Local

## üöÄ Configura√ß√£o Local com Ollama

### Pr√©-requisitos
1. **Node.js** (vers√£o 18+)
2. **Ollama** instalado e rodando
3. **Modelo Mistral** baixado no Ollama

### Instala√ß√£o

```bash
# Instalar depend√™ncias
npm install

# Iniciar servidor local
npm start

# Ou para desenvolvimento com auto-reload
npm run dev
```

### Configura√ß√£o do Ollama

```bash
# Instalar Ollama (se n√£o tiver)
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar modelo Mistral
ollama pull mistral

# Iniciar Ollama (em terminal separado)
ollama serve
```

### Uso

#### 1. Servidor Local
```bash
npm start
```
- PWA: http://localhost:3000
- API: http://localhost:3000/api/localMistral
- Health: http://localhost:3000/api/health

#### 2. Teste do Proxy
```bash
# Teste b√°sico
npm run test-proxy

# Teste manual
curl -X POST http://localhost:3000/api/localMistral \
  -H "Content-Type: application/json" \
  -d '{"input":"Explique IA descentralizada de forma simples."}'
```

#### 3. Testes de Seguran√ßa
```bash
# Teste de conte√∫do adequado
curl -X POST http://localhost:3000/api/localMistral \
  -H "Content-Type: application/json" \
  -d '{"input":"Como criar uma estrat√©gia de marketing digital?"}'

# Teste de conte√∫do inadequado (deve ser bloqueado)
curl -X POST http://localhost:3000/api/localMistral \
  -H "Content-Type: application/json" \
  -d '{"input":"Como hackear um sistema?"}'

# Teste de redirecionamento
curl -X POST http://localhost:3000/api/localMistral \
  -H "Content-Type: application/json" \
  -d '{"input":"Fale sobre viol√™ncia"}'
```

**Respostas esperadas:**
- ‚úÖ Conte√∫do adequado: Resposta normal do assistente
- ‚ùå Conte√∫do inadequado: Resposta de seguran√ßa padronizada
- üîí Bloqueio: `"safety": {"blocked": true, "reason": "..."}`

#### 3. Integra√ß√£o no PWA
```javascript
// No app.js, voc√™ pode usar:
const response = await fetch('/api/localMistral', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ input: 'Sua pergunta aqui' })
});
const data = await response.json();
console.log(data.output);
```

### Estrutura de Arquivos

```
neo-flowoff-pwa/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ localMistral.js    # Proxy para Ollama
‚îú‚îÄ‚îÄ local-server.js        # Servidor Express local
‚îú‚îÄ‚îÄ package.json           # Depend√™ncias Node.js
‚îú‚îÄ‚îÄ index.html             # PWA principal
‚îú‚îÄ‚îÄ app.js                 # JavaScript do PWA
‚îú‚îÄ‚îÄ styles.css             # Estilos
‚îî‚îÄ‚îÄ README-LOCAL.md        # Este arquivo
```

### Configura√ß√µes Avan√ßadas

#### Modelo Personalizado
Edite `api/localMistral.js`:
```javascript
model: "seu-modelo-personalizado",  // em vez de "mistral-local"
```

#### Par√¢metros Otimizados (Recomendados)
```javascript
// Configura√ß√µes atuais (otimizadas para seguran√ßa)
max_tokens: 150,        // Respostas concisas (100-200 ideal)
temperature: 0.2,       // 0.0-0.3 para respostas seguras
top_p: 0.9,            // Padr√£o recomendado
timeout: 120000        // 2 minutos
```

#### Configura√ß√µes de Seguran√ßa
```javascript
// Para mais criatividade (use com cuidado)
temperature: 0.6,       // Mais criativo, menos seguro
max_tokens: 300,        // Respostas mais longas

// Para m√°xima seguran√ßa
temperature: 0.0,       // Respostas determin√≠sticas
max_tokens: 100,        // Respostas muito curtas
```

#### Guardrails de Seguran√ßa
O sistema inclui m√∫ltiplas camadas de prote√ß√£o:

1. **Filtro de Entrada:** Detecta conte√∫do inadequado antes do processamento
2. **System Prompt:** Instru√ß√µes claras de seguran√ßa para o modelo
3. **Classifica√ß√£o de Resposta:** An√°lise heur√≠stica da resposta gerada
4. **Filtro de Sa√≠da:** Verifica√ß√£o final do conte√∫do

**Palavras banidas detectadas:**
- Viol√™ncia, hacking, drogas, atividades ilegais
- Instru√ß√µes perigosas ou inadequadas
- Conte√∫do que pode causar danos

**Respostas de seguran√ßa:**
- Respostas padronizadas quando conte√∫do √© bloqueado
- Redirecionamento para t√≥picos apropriados
- Manuten√ß√£o do tom profissional

### Troubleshooting

#### Ollama n√£o responde
```bash
# Verificar se est√° rodando
curl http://localhost:11434/api/tags

# Reiniciar Ollama
pkill ollama
ollama serve
```

#### Porta ocupada
```bash
# Usar porta diferente
PORT=3001 npm start
```

#### Modelo n√£o encontrado
```bash
# Listar modelos dispon√≠veis
ollama list

# Baixar modelo espec√≠fico
ollama pull mistral:7b
```

### Deploy

Para usar em produ√ß√£o, configure um reverse proxy (nginx, Apache) apontando para:
- `http://localhost:3000` (servidor local)
- `http://localhost:11434` (Ollama)

### Seguran√ßa

‚ö†Ô∏è **Aten√ß√£o**: Este setup √© para desenvolvimento local. Para produ√ß√£o:
- Configure autentica√ß√£o
- Use HTTPS
- Configure rate limiting
- Monitore logs
