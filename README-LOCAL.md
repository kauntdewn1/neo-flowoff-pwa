# NEO FlowOff PWA - Servidor Local

## üöÄ Configura√ß√£o Local com Ollama

### Pr√©-requisitos
1. **Node.js** (vers√£o 18+)
2. **Ollama** instalado e rodando
3. **Modelo Mistral** baixado no Ollama

### Instala√ß√£o

```bash
# Instalar depend√™ncias
npm install

# Iniciar servidor local
npm start

# Ou para desenvolvimento com auto-reload
npm run dev
```

### Configura√ß√£o do Ollama

```bash
# Instalar Ollama (se n√£o tiver)
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar modelo Mistral
ollama pull mistral

# Iniciar Ollama (em terminal separado)
ollama serve
```

### Uso

#### 1. Servidor Local
```bash
npm start
```
- PWA: http://localhost:3000
- API: http://localhost:3000/api/localMistral
- Health: http://localhost:3000/api/health

#### 2. Teste do Proxy
```bash
# Teste b√°sico
npm run test-proxy

# Teste manual
curl -X POST http://localhost:3000/api/localMistral \
  -H "Content-Type: application/json" \
  -d '{"input":"Explique IA descentralizada de forma simples."}'
```

#### 3. Integra√ß√£o no PWA
```javascript
// No app.js, voc√™ pode usar:
const response = await fetch('/api/localMistral', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ input: 'Sua pergunta aqui' })
});
const data = await response.json();
console.log(data.output);
```

### Estrutura de Arquivos

```
neo-flowoff-pwa/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ localMistral.js    # Proxy para Ollama
‚îú‚îÄ‚îÄ local-server.js        # Servidor Express local
‚îú‚îÄ‚îÄ package.json           # Depend√™ncias Node.js
‚îú‚îÄ‚îÄ index.html             # PWA principal
‚îú‚îÄ‚îÄ app.js                 # JavaScript do PWA
‚îú‚îÄ‚îÄ styles.css             # Estilos
‚îî‚îÄ‚îÄ README-LOCAL.md        # Este arquivo
```

### Configura√ß√µes Avan√ßadas

#### Modelo Personalizado
Edite `api/localMistral.js`:
```javascript
model: "seu-modelo-personalizado",  // em vez de "mistral-local"
```

#### Timeout e Par√¢metros
```javascript
max_tokens: 500,        // Respostas mais longas
temperature: 0.7,       // Mais criativo
timeout: 180000        // 3 minutos
```

### Troubleshooting

#### Ollama n√£o responde
```bash
# Verificar se est√° rodando
curl http://localhost:11434/api/tags

# Reiniciar Ollama
pkill ollama
ollama serve
```

#### Porta ocupada
```bash
# Usar porta diferente
PORT=3001 npm start
```

#### Modelo n√£o encontrado
```bash
# Listar modelos dispon√≠veis
ollama list

# Baixar modelo espec√≠fico
ollama pull mistral:7b
```

### Deploy

Para usar em produ√ß√£o, configure um reverse proxy (nginx, Apache) apontando para:
- `http://localhost:3000` (servidor local)
- `http://localhost:11434` (Ollama)

### Seguran√ßa

‚ö†Ô∏è **Aten√ß√£o**: Este setup √© para desenvolvimento local. Para produ√ß√£o:
- Configure autentica√ß√£o
- Use HTTPS
- Configure rate limiting
- Monitore logs
